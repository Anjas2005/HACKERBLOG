package main

import (
	"encoding/csv"
	"fmt"
	"log"
	"net/http" // For HTTP req and response
	"os"

	"github.com/PuerkitoBio/goquery"
)

// first create a global type for the scraped data
type Product struct{
		name,price string
	}
func main(){
	// Download the target html document
	res, err := http.Get("https://www.scrapingcourse.com/ecommerce/")
	if err != nil{
		log.Fatal("Failed to connect to the target page", err)
	}
	defer res.Body.Close()
	if res.StatusCode != 200{
		log.Fatalf("HTTP Error %d: %s",res.StatusCode,res.Status)
	}

	// fmt.Println(res.Body) 
	// This will output &{[] {0xc000144480} <nil> <nil>} Not the html 
	// because its and io.ReadCloser Buffer not the HTML ITSELF

	// The Below Fxn will convert it into HTML 	

	// Convert the response to bytesconst
	// bytebody, err := io.ReadAll(res.Body)
	// if err != nil{
	// 	log.Fatal("Error while Reading the Response Buffer",err)
	// }

	// //convert the byte HTML content to string and print it
	// html:= string(bytebody)
	// fmt.Println(html)


	// Now we have taken in the HTML but now we need to use goquery to only take the required parts 
	// go query is an html parsing library and can be installed using 
	// go get github.com/PuerkitoBio/goquery
	// below code will parse the HTML document
	doc,err := goquery.NewDocumentFromReader(res.Body)
	if err!= nil {
		log.Fatal("Failed to parse the HTML document",err)
	}

	// Extracting the first item from the webiste https://www.scrapingcourse.com/ecommerce/
	productHTMLElement:= doc.Find("li.product").First() //selects the first element in li.product  
	name:= productHTMLElement.Find("h2").Text()
	fmt.Println(name)

	// ok now we will extract all the required data at once
	// This is an array of type Product which will contain all the scraped data
	var products []Product


// .Each(func(i int, p * goquery.Selection )
// 	Step-by-Step Breakdown
// Find all the products:
// doc.Find("li.product") grabs every <li class="product"> from the page.

// Loop through each product:
// .Each(...) goes through them one by one.

// For each product:

// i tells you which product youâ€™re on (first, second, third, etc.).

// p lets you look at that product and do something (like read its name).


	doc.Find("li.product").Each(func(i int, p * goquery.Selection ) {
		//Scraping Logic
		product := Product{}
		product.name = p.Find("h2").Text()
		product.price = p.Find("span.price").Text()

		//Store the Scraped Product

		products= append(products, product)
	})

	fmt.Println(products)


	// Export the output to a CSV File
	// initialize the output csv file
	file, err:= os.Create("products.csv")
	if err!= nil{
		log.Fatal("Failed to create the output CSV file",err)
	}
	defer file.Close()

	// Initialize a file writer
	writer := csv.NewWriter(file) //creates a new csv writer that will encode the data into your file
	defer writer.Flush() // The data is first saved into some sort of cache and then transferred to the disk
													// but this ensuers that the code doesnt stop or proceed until the data is completly transferred in the disk
	//define the CSV headers
	headers := []string{
		"image",
		"price",
	}
	//write coloumn headers
	writer.Write(headers)

	// add each product to the CSV file
	for _,product := range products {
		// Convert a Product to an Array of Strings
		record := []string{
			product.name,
			product.price,
		}
		// Write a new CSV record
		writer.Write(record)
	} // Initialize the output CSV File
}
